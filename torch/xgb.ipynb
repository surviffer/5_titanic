{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:white;margin:0;font-size:270%;text-align:center;display:fill;border-radius:5px;background-color:#4f4e4e;overflow:hidden;font-weight:500\">Extreme Gradient Boosting (XGBoost) </br> Wrangling with Hyperparameters</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\">Extreme Gradient Boosting (XGBoost)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßæ 1. Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala. It works on Linux, Windows, and macOS. From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting Library\".\n",
    "\n",
    "Extreme Gradient Boosting Algorithm. Gradient boosting refers to **a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems**. **Ensembles are constructed from decision tree models**.\n",
    "\n",
    "**XGBoost is a more regularized form of Gradient Boosting**. XGBoost uses advanced regularization (L1 & L2), which improves model generalization capabilities. XGBoost delivers high performance as compared to Gradient Boosting. Its training is very fast and can be parallelized across clusters.\n",
    "\n",
    "Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://cdn.educba.com/academy/wp-content/uploads/2019/06/XGBoost-Algorithm1.jpg\" />\n",
    "</div>\n",
    "\n",
    "*educba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\"> Introduction to Hyperparameters</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†  2. Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is meant by hyperparameter tuning?\n",
    "In machine learning, **hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm**. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned. The hyper-parameter tuning process is a tightrope walk to achieve a balance between underfitting and overfitting. Underfitting is when the machine learning model is unable to reduce the error for either the test or training set.\n",
    "\n",
    "### What is hyperparameter tuning example?\n",
    "Some examples of model hyperparameters include: The penalty in Logistic Regression Classifier i.e. L1 or L2 regularization. The learning rate for training a neural network. The C and sigma hyperparameters for support vector machines.\n",
    "\n",
    "### Why do we use hyperparameter tuning?\n",
    "Hyperparameter tuning is an essential part of controlling the behavior of a machine learning model. If we don't correctly tune our hyperparameters, our estimated model parameters produce suboptimal results, as they don't minimize the loss function. This means our model makes more errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\"> XGBoost Parameters - Classification</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚åõÔ∏è 3. XGBoost Hyperparameters for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>XGBoost Boosters</b></div>\n",
    "\n",
    "#### -> gbtree :\n",
    " - **gbtree** for **tree-based models**;  \n",
    " \n",
    "#### -> gblinear :\n",
    " - **gblinear** for **linear models** to run at each iteration.\n",
    "\n",
    "#### -> dart :\n",
    " - **dart** is also a tree based model. \n",
    " - XGBoost mostly combines a huge number of regression trees with a small learning rate. In this situation, trees added early are significant and trees added late are unimportant. *Vinayak and Gilad-Bachrach* proposed a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations - it s called dart. It drops trees in order to solve the over-fitting. Trivial trees (to correct trivial errors) may be prevented. Because of the randomness introduced in the training, expect the following few differences: Training can be slower than gbtree because the random dropout prevents usage of the prediction buffer. The early stop might not be stable, due to the randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>General parameters</b></div>\n",
    "It relates to which booster we are using to do boosting, commonly tree or linear model\n",
    "‚Äã\n",
    "#### 1. booster [default=gbtree]\n",
    " - **gbtree** for **tree-based models**;  **gblinear** for **linear models** to run at each iteration.\n",
    " \n",
    "#### 2. silent [default=0]:\n",
    " - **To activate Silent silent mode, set it to 1,meaning off; so, no messages will be printed.**\n",
    "‚Äã\n",
    "It‚Äôs generally a good idea, to keep it 0 as the messages might help in understanding the model; and how the metrics are going.\n",
    "‚Äã\n",
    "#### 3. nthread [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered\n",
    "If you wish to run on all cores, value should not be entered and algorithm will detect automatically\n",
    "‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Booster parameters</b></div>\n",
    "\n",
    "depend on which booster you have chosen. We will discuss about tree based boosters here.\n",
    "\n",
    "#### 1. eta [default=0.3]\n",
    "- Similiar to **learning_rate**; works with geeneralization.\n",
    "- **Typical final values to be used:** 0.01-0.2\n",
    "\n",
    "#### 2. min_child_weight [default=1]\n",
    "- Defines the **minimum sum of weights of all observations required in a child**. It is **used to control over-fitting**. Higher values prevent a model from learning - relations which might be highly specific to the particular sample selected for a tree.\n",
    "- Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "#### 3. max_depth [default=6]\n",
    "- The **maximum depth of a tree**, it is also used **to control over-fitting** as higher depth will allow model to learn relations very specific to a particular sample. Should be tuned using CV.\n",
    "- **Typical values:** 3-10\n",
    "\n",
    "#### 4. max_leaf_nodes\n",
    "- The **maximum number of terminal nodes** or leaves in a tree.\n",
    "- Since binary trees are created, a depth of ‚Äòn‚Äô would produce a maximum of 2^n leaves. If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "#### 5. gamma [default=0]\n",
    "- A node is split only when the resulting split gives a positive reduction in the loss function. **Gamma specifies the minimum loss reduction required for a split to occur.**\n",
    "- Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned. **Can be used to control overfitting.**\n",
    "\n",
    "#### 6. max_delta_step [default=0]\n",
    "- In **maximum delta step we allow each tree‚Äôs weight estimation to be**. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "- This is generally not used.\n",
    "\n",
    "#### 7. subsample [default=1]\n",
    "- It denotes the **fraction of observations to be randomly samples for each tree**.\n",
    "- **Lower values make the algorithm more conservative and prevents overfitting** but too small values might lead to under-fitting.\n",
    "- **Typical values:** 0.5-1\n",
    "\n",
    "#### 8. colsample_bytree [default=1]\n",
    "- Denotes the **fraction of columns to be randomly samples for each tree**. Smaller colsample_bytree povides additional regulerization.\n",
    "- **Typical values:** 0.5-1\n",
    "\n",
    "#### 9. colsample_bylevel [default=1]\n",
    "- Denotes **the subsample ratio of columns for each split, in each level**.\n",
    "\n",
    "#### 10. alpha [default=0]\n",
    "- **L1 regularization term**.\n",
    "- L1 regularization forces the weights of uninformative features to be zero by substracting a small amount from the weight at each iteration and thus making the weight zero, eventually. It is also called regularization for simplicity. Applies on leaf weights (rather than feature weights); larger value mean more regularization. \n",
    "\n",
    "#### 11. lambda [default=1]\n",
    "- **L2 regularization term**.\n",
    "- This used to handle the regularization part of XGBoost. It should be explored to reduce overfitting.\n",
    "- L2 regularization acts like a force that removes a small percentage of weights at each iteration. Therefore, weights will never be equal to zero. L2 regularization penalizes (weight)¬≤ There is an additional parameter to tune the L2 regularization term which is called regularization rate (lambda).\n",
    "- Smoother than alpha. Also, applies on leaf weights.\n",
    "\n",
    "#### 12. scale_pos_weight [default=1]\n",
    "- A value **greater than 0 should be used** in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Learning task parameters</b></div>\n",
    "\n",
    "decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\n",
    "\n",
    "#### 1. objective [default=reg:linear]\n",
    "- This **defines the loss function to be minimized**. \n",
    "- Mostly used values are:\n",
    " - **reg:linear** - used for regressions\n",
    " - **reg:logistic** - used for classification, when you want the decision only, not the probability.\n",
    " - **binary:logistic** ‚Äìlogistic regression for binary classification, returns predicted probability (rather than decision class) \n",
    " - **multi:softmax** ‚Äìmulticlass classification using the softmax objective, returns predicted class (not probabilities); you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    " - **multi:softprob** ‚Äìsame as softmax, but returns predicted probability of each data point belonging to each class.\n",
    " \n",
    "#### 2. eval_metric [ default according to objective ]\n",
    "- The metric to be used for validation data.\n",
    "- The default values are rmse for regression and error for classification.\n",
    "- **Typical values are:**\n",
    " - **rmse** ‚Äì root mean square error\n",
    " - **mae** ‚Äì mean absolute error\n",
    " - **logloss** ‚Äì negative log-likelihood\n",
    " - **error** ‚Äì Binary classification error rate (0.5 threshold)\n",
    " - **merror** ‚Äì Multiclass classification error rate\n",
    " - **mlogloss** ‚Äì Multiclass logloss\n",
    " - **auc** ‚Äì Area under the curve\n",
    " \n",
    "#### 3. seed [default=0]\n",
    "- for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Command line parameters</b></div>\n",
    "\n",
    "- relate to behavior of CLI version of XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\">Understanding Bias-Variance Tradeoff</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ 4. Understanding Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a machine learning or statistics course, this is likely to be one of the most important concepts. When we allow the model to get more complicated (e.g. more depth), the model has better ability to fit the training data, resulting in a less biased model. However, such complicated model requires more data to fit.\n",
    "\n",
    "Most of parameters in XGBoost are about bias variance tradeoff. The best model should trade the model complexity with its predictive power carefully. Parameters Documentation will tell you whether each parameter will make the model more conservative or not. This can be used to help you turn the knob between complicated model and simple model.\n",
    "\n",
    "## More Resources :\n",
    "- [üìã Bias-Variance Tradeoff ‚û°Ô∏è with NumPy & Seaborn](https://www.kaggle.com/code/azminetoushikwasi/bias-variance-tradeoff-with-numpy-seaborn)\n",
    "- [Mastering Bias-Variance Tradeoff wih Polynomials](https://medium.com/@azmine_wasi/mastering-bias-variance-tradeoff-with-polynomials-part-02-29f9bb53bb26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import json\n",
    "from sklearn import manifold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import plot_tree\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('../spaceship_titanic/spaceship-titanic/train.csv')\n",
    "df_test=pd.read_csv('../spaceship_titanic/spaceship-titanic/test.csv')\n",
    "def info_of_dataset(df):\n",
    "    df1=df.dtypes\n",
    "    df2=df.isnull().sum()\n",
    "    df3=df.isnull().sum()/df.shape[1]\n",
    "    df4=pd.DataFrame([df1,df2,df3])\n",
    "    return df4.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HomePlanet</th>\n",
       "      <td>object</td>\n",
       "      <td>201</td>\n",
       "      <td>14.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CryoSleep</th>\n",
       "      <td>object</td>\n",
       "      <td>217</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>object</td>\n",
       "      <td>199</td>\n",
       "      <td>14.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destination</th>\n",
       "      <td>object</td>\n",
       "      <td>182</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>float64</td>\n",
       "      <td>179</td>\n",
       "      <td>12.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIP</th>\n",
       "      <td>object</td>\n",
       "      <td>203</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoomService</th>\n",
       "      <td>float64</td>\n",
       "      <td>181</td>\n",
       "      <td>12.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FoodCourt</th>\n",
       "      <td>float64</td>\n",
       "      <td>183</td>\n",
       "      <td>13.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShoppingMall</th>\n",
       "      <td>float64</td>\n",
       "      <td>208</td>\n",
       "      <td>14.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spa</th>\n",
       "      <td>float64</td>\n",
       "      <td>183</td>\n",
       "      <td>13.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRDeck</th>\n",
       "      <td>float64</td>\n",
       "      <td>188</td>\n",
       "      <td>13.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>object</td>\n",
       "      <td>200</td>\n",
       "      <td>14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transported</th>\n",
       "      <td>bool</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0    1          2\n",
       "PassengerId    object    0        0.0\n",
       "HomePlanet     object  201  14.357143\n",
       "CryoSleep      object  217       15.5\n",
       "Cabin          object  199  14.214286\n",
       "Destination    object  182       13.0\n",
       "Age           float64  179  12.785714\n",
       "VIP            object  203       14.5\n",
       "RoomService   float64  181  12.928571\n",
       "FoodCourt     float64  183  13.071429\n",
       "ShoppingMall  float64  208  14.857143\n",
       "Spa           float64  183  13.071429\n",
       "VRDeck        float64  188  13.428571\n",
       "Name           object  200  14.285714\n",
       "Transported      bool    0        0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_of_dataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df=df.drop(['Name','PassengerId'],axis=1)\n",
    "    df=df.dropna()\n",
    "    \n",
    "    target=df['Transported']\n",
    "    df=df.drop(['Transported'],axis=1)\n",
    "    target = target.astype(int)\n",
    "    \n",
    "    df['Cabin_1']= df['Cabin'].str[0]\n",
    "    df['Cabin_2']= df['Cabin'].str[2]\n",
    "    df['Cabin_3']= df['Cabin'].str[5]\n",
    "    df=df.drop(['Cabin'],axis=1)\n",
    "                                             \n",
    "    \n",
    "    # Create the training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state=100,\n",
    "                                                    stratify=target)\n",
    "\n",
    "    numaric_columns=list(df.select_dtypes(include=np.number).columns)\n",
    "    print(\"Numaric columns (\"+str(len(numaric_columns))+\") :\",\", \".join(numaric_columns))\n",
    "    \n",
    "    cat_columns=df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(\"Categorical columns (\"+str(len(cat_columns))+\") :\",\", \".join(cat_columns))\n",
    "    \n",
    "    \n",
    "    X_train_n=X_train[numaric_columns]\n",
    "    X_test_n=X_test[numaric_columns]\n",
    "    \n",
    "    X_train_c=X_train[cat_columns]\n",
    "    X_test_c=X_test[cat_columns]\n",
    "                               \n",
    "   \n",
    "    encoder=OrdinalEncoder()#È°∫Â∫èÁºñÁ†Å\n",
    "    X_train_c = encoder.fit_transform(X_train_c)\n",
    "    X_train_c=pd.DataFrame(X_train_c)\n",
    "    X_test_c = encoder.transform(X_test_c)\n",
    "    X_test_c=pd.DataFrame(X_test_c)\n",
    "    \n",
    "    i=1\n",
    "    for column in X_train_c:\n",
    "        X_train_n[\"cat_\"+str(i)]=X_train_c[column]\n",
    "        X_test_n[\"cat_\"+str(i)]=X_test_c[column]\n",
    "        i=i+1\n",
    "    \n",
    "    #X_train=pd.concat([X_train_n,X_train_c],axis=1,ignore_index=True)\n",
    "    #X_test=pd.concat([X_test_n,X_test_c],axis=1,ignore_index=True)\n",
    "    \n",
    "    X_train_n=X_train_n.fillna(X_train_n.mean())\n",
    "    X_test_n=X_test_n.fillna(X_test_n.mean())\n",
    "    \n",
    "    \n",
    "    return X_train_n, X_test_n, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Preparing functions for making things easier later</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numaric columns (6) : Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck\n",
      "Categorical columns (7) : HomePlanet, CryoSleep, Destination, VIP, Cabin_1, Cabin_2, Cabin_3\n",
      "(5411, 13) (1353, 13) (5411,) (1353,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=process_df(df_train)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>1.467466</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>4.265967</td>\n",
       "      <td>3.250675</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>29.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4940.0</td>\n",
       "      <td>1831.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>25.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>18.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1745.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>1.467466</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>4.265967</td>\n",
       "      <td>3.250675</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7832</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>1.467466</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>4.265967</td>\n",
       "      <td>3.250675</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>1.467466</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>4.265967</td>\n",
       "      <td>3.250675</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>18.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>1.467466</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>4.265967</td>\n",
       "      <td>3.250675</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>27.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2948.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.208865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck     cat_1  \\\n",
       "6279  29.0          2.0        3.0        1501.0    26.0     0.0  0.655172   \n",
       "4457  29.0        574.0        0.0        4940.0  1831.0     2.0  1.000000   \n",
       "3518  42.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "4668  46.0          0.0        4.0         834.0     0.0    32.0  0.000000   \n",
       "4947  25.0        662.0       34.0           0.0    79.0   148.0  0.000000   \n",
       "4891  25.0         10.0        0.0         908.0     5.0     0.0  1.000000   \n",
       "1766  49.0          0.0        0.0           0.0   724.0     0.0  1.000000   \n",
       "4415  35.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "3659  18.0        659.0        0.0        2316.0     0.0    23.0  0.000000   \n",
       "684   38.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "2835  39.0          0.0        0.0           0.0  1745.0    72.0  2.000000   \n",
       "1879   2.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "6546  29.0          0.0        0.0           0.0     0.0     0.0  0.655172   \n",
       "7832  25.0          0.0     1673.0           0.0   642.0   612.0  0.655172   \n",
       "6818  24.0          1.0      194.0           0.0     0.0   632.0  0.655172   \n",
       "6036  18.0        358.0      336.0           0.0     0.0     0.0  0.655172   \n",
       "2703  27.0        688.0      307.0         779.0     0.0    19.0  1.000000   \n",
       "3311  37.0         78.0        0.0           0.0  2948.0     0.0  1.000000   \n",
       "1310  38.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "2408  29.0          0.0        0.0           0.0     0.0     0.0  1.000000   \n",
       "\n",
       "         cat_2     cat_3    cat_4     cat_5     cat_6      cat_7  \n",
       "6279  0.351124  1.467466  0.02099  4.265967  3.250675   3.208865  \n",
       "4457  1.000000  2.000000  0.00000  1.000000  8.000000  12.000000  \n",
       "3518  0.000000  0.000000  0.00000  2.000000  3.000000   0.000000  \n",
       "4668  0.000000  2.000000  0.00000  5.000000  9.000000   0.000000  \n",
       "4947  1.000000  2.000000  0.00000  6.000000  1.000000  10.000000  \n",
       "4891  1.000000  0.000000  0.00000  1.000000  3.000000   0.000000  \n",
       "1766  0.000000  2.000000  0.00000  2.000000  1.000000   0.000000  \n",
       "4415  1.000000  2.000000  0.00000  2.000000  2.000000   0.000000  \n",
       "3659  1.000000  1.000000  0.00000  6.000000  7.000000   0.000000  \n",
       "684   0.000000  0.000000  0.00000  2.000000  2.000000   0.000000  \n",
       "2835  1.000000  2.000000  0.00000  4.000000  1.000000  12.000000  \n",
       "1879  0.000000  2.000000  0.00000  1.000000  1.000000   0.000000  \n",
       "6546  0.351124  1.467466  0.02099  4.265967  3.250675   3.208865  \n",
       "7832  0.351124  1.467466  0.02099  4.265967  3.250675   3.208865  \n",
       "6818  0.351124  1.467466  0.02099  4.265967  3.250675   3.208865  \n",
       "6036  0.351124  1.467466  0.02099  4.265967  3.250675   3.208865  \n",
       "2703  0.000000  0.000000  0.00000  2.000000  2.000000   0.000000  \n",
       "3311  0.000000  0.000000  0.00000  1.000000  1.000000   0.000000  \n",
       "1310  1.000000  2.000000  0.00000  1.000000  3.000000   0.000000  \n",
       "2408  1.000000  2.000000  0.00000  1.000000  3.000000   3.208865  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"xgb\", entity=\"surviffer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base parameters : \n",
    "- booster = \"gbtree\",      **as it is a classification problem**\n",
    "- objective = \"binary:logistic\",    **as per the problem**\n",
    "- tree_method=\"gpu_hist\"   **using GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_helper(PARAMETERS,V_PARAM_NAME=False,V_PARAM_VALUES=False,BR=10):\n",
    "    \n",
    "    temp_dmatrix =xgb.DMatrix(data=X_train, label=y_train)\n",
    "    \n",
    "    if V_PARAM_VALUES==False:\n",
    "        cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=BR,params=PARAMETERS, as_pandas=True, seed=123 )\n",
    "        return cv_results\n",
    "    \n",
    "    else:\n",
    "        results=[]\n",
    "        \n",
    "        for v_param_value in V_PARAM_VALUES:\n",
    "            PARAMETERS[V_PARAM_NAME]=v_param_value\n",
    "            cv_results = xgb.cv(dtrain=temp_dmatrix, nfold=5,num_boost_round=BR,params=PARAMETERS, as_pandas=True, seed=123)\n",
    "            results.append((cv_results[\"train-auc-mean\"].tail().values[-1],cv_results[\"test-auc-mean\"].tail().values[-1]))\n",
    "            \n",
    "        data = list(zip(V_PARAM_VALUES, results))\n",
    "        print(pd.DataFrame(data,columns=[V_PARAM_NAME,\"auc\"]))\n",
    "        \n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Create a general base model and evaluate performance</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858413</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.005628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.870282</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.828690</td>\n",
       "      <td>0.004939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.876322</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.835879</td>\n",
       "      <td>0.002703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.880106</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.838758</td>\n",
       "      <td>0.004261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.884137</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.838674</td>\n",
       "      <td>0.003967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.887057</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.841544</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.889695</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.842214</td>\n",
       "      <td>0.005057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891214</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.842708</td>\n",
       "      <td>0.005743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.892886</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.843644</td>\n",
       "      <td>0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.893928</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.844092</td>\n",
       "      <td>0.005543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0        0.858413       0.002187       0.821862      0.005628\n",
       "1        0.870282       0.001689       0.828690      0.004939\n",
       "2        0.876322       0.000856       0.835879      0.002703\n",
       "3        0.880106       0.001456       0.838758      0.004261\n",
       "4        0.884137       0.001624       0.838674      0.003967\n",
       "5        0.887057       0.001697       0.841544      0.003510\n",
       "6        0.889695       0.002341       0.842214      0.005057\n",
       "7        0.891214       0.002497       0.842708      0.005743\n",
       "8        0.892886       0.002178       0.843644      0.005887\n",
       "9        0.893928       0.002417       0.844092      0.005543"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\"}\n",
    "xgb_helper(PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Optimizing number of boosting rounds</br>(as we will be using DMatrix from xgb)</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds       auc\n",
      "0                    5  0.840033\n",
      "1                   10  0.843594\n",
      "2                   15  0.845747\n",
      "3                   20  0.845400\n",
      "4                   25  0.844094\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix =xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"binary:logistic\", \"max_depth\":5}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15, 20, 25]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=5, num_boost_round=curr_num_rounds, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-auc-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"auc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking num_boosting_rounds = 10; to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>   1. Choose the learning rate. Maybe you can start with a higher one. 0.5 - 0.1 is ok for starting in most cases.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858413</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.005628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.872958</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.831666</td>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.879817</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.836776</td>\n",
       "      <td>0.005321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.884087</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.838199</td>\n",
       "      <td>0.005545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.888099</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.840862</td>\n",
       "      <td>0.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.890920</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.841520</td>\n",
       "      <td>0.005769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.893914</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.842223</td>\n",
       "      <td>0.005199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.896395</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.842609</td>\n",
       "      <td>0.005717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.897477</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.842670</td>\n",
       "      <td>0.005631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.900422</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.842716</td>\n",
       "      <td>0.005787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0        0.858413       0.002187       0.821862      0.005628\n",
       "1        0.872958       0.002187       0.831666      0.004109\n",
       "2        0.879817       0.001506       0.836776      0.005321\n",
       "3        0.884087       0.002211       0.838199      0.005545\n",
       "4        0.888099       0.002316       0.840862      0.006667\n",
       "5        0.890920       0.002014       0.841520      0.005769\n",
       "6        0.893914       0.001330       0.842223      0.005199\n",
       "7        0.896395       0.002359       0.842609      0.005717\n",
       "8        0.897477       0.002852       0.842670      0.005631\n",
       "9        0.900422       0.002209       0.842716      0.005787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5}\n",
    "xgb_helper(PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- A good starting Classifier, with **train-auc-mean of 0.900422, test-auc-mean of 0.842716. Let;s keep tuning and reduce overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  2. Using CV, tune max_depth and min_child_weight next.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  2.1. Tuning max_depth.</b></div>\n",
    "\n",
    "**Tips:** Keep it around 3-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth                                       auc\n",
      "0          3  (0.8643899441624237, 0.8434407018878254)\n",
      "1          4  (0.8762391828253827, 0.8432457875673736)\n",
      "2          5  (0.8898517716460612, 0.8449580853400329)\n",
      "3          6  (0.9004222967227931, 0.8427158084686912)\n",
      "4          7  (0.9107993432039982, 0.8379593667718235)\n",
      "5          8  (0.9233976130022082, 0.8309442810154544)\n",
      "6          9  (0.9331601626845524, 0.8328975608830239)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5}\n",
    "V_PARAM_NAME=\"max_depth\"\n",
    "V_PARAM_VALUES=range(3,10,1)\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking max_depth 5, as per the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  2.2. Tuning min_child_weigh.</b></div>\n",
    "\n",
    "**Tips:** Keep it small for imbalanced datasets,good for balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_child_weight                                       auc\n",
      "0                 0  (0.8923932136315347, 0.8400307515925263)\n",
      "1                 1  (0.8898517716460612, 0.8449580853400329)\n",
      "2                 2  (0.8878844356167308, 0.8430174554684318)\n",
      "3                 3   (0.8842988914848681, 0.842868768786811)\n",
      "4                 4  (0.8841976959835126, 0.8426672020116197)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5}\n",
    "V_PARAM_NAME=\"min_child_weight\"\n",
    "V_PARAM_VALUES=range(0,5,1)\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking min_child_weight 1, as per the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  3. Its time for gamma.</b></div>\n",
    "\n",
    "**Tips:** Keep it small like 0.1-0.2 forstarting. Will be tuned later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gamma                                       auc\n",
      "0    0.1   (0.889994877151358, 0.8443115370456209)\n",
      "1    0.2  (0.8908020107961383, 0.8447684831269007)\n",
      "2    0.5   (0.888948000423167, 0.8450077335411604)\n",
      "3    1.0   (0.888184172336776, 0.8451616022595031)\n",
      "4    1.5  (0.8875863399894248, 0.8435783340359313)\n",
      "5    2.0  (0.8861811009373095, 0.8449147712912233)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1}\n",
    "V_PARAM_NAME = \"gamma\"\n",
    "V_PARAM_VALUES = [0.1,0.2,0.5,1,1.5,2]\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking gamma t 1, as per the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  4. Tune subsample and colsample_bytree.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  4.1. Tuning subsample.</b></div>\n",
    "\n",
    "**Tips:** Keep it small in range 0.5-0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subsample                                       auc\n",
      "0        0.4    (0.8749532101660904, 0.83281692487631)\n",
      "1        0.5  (0.8789542430190034, 0.8356286366243391)\n",
      "2        0.6  (0.8804439995005579, 0.8371190653296372)\n",
      "3        0.7  (0.8852418637174774, 0.8388110573107215)\n",
      "4        0.8  (0.8868771320489373, 0.8385871061415084)\n",
      "5        0.9  (0.8880784719598278, 0.8414592031099557)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1,\"gamma\":1}\n",
    "V_PARAM_NAME = \"subsample\"\n",
    "V_PARAM_VALUES = [.4,.5,.6,.7,.8,.9]\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  4.2. Tune colsample_bytree.</b></div>\n",
    "\n",
    "**Tips:** Keep it small in range 0.5-0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree                                       auc\n",
      "0               0.4  (0.8732754700508629, 0.8243029336177725)\n",
      "1               0.5  (0.8763308230288717, 0.8286368940154538)\n",
      "2               0.6  (0.8797842450245362, 0.8345199439450719)\n",
      "3               0.7   (0.882606499584198, 0.8387111950766428)\n",
      "4               0.8  (0.8816423223073414, 0.8372514630828988)\n",
      "5               0.9   (0.882945245407946, 0.8377939587450118)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1,\"gamma\":1,\"subsample\":0.7}\n",
    "V_PARAM_NAME = \"colsample_bytree\"\n",
    "V_PARAM_VALUES = [.4,.5,.6,.7,.8,.9]\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  4.3. Tune scale_pos_weight.</b></div>\n",
    "\n",
    "**Tips:** Based on class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   scale_pos_weight                                       auc\n",
      "0               0.5  (0.8788106918058312, 0.8392510901997741)\n",
      "1               1.0   (0.882606499584198, 0.8387111950766428)\n",
      "2               2.0  (0.8820958822662963, 0.8372847814140968)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1,\n",
    "            \"gamma\":1,\"subsample\":0.7,\"colsample_bytree\":.7}\n",
    "\n",
    "V_PARAM_NAME = \"scale_pos_weight\"\n",
    "V_PARAM_VALUES = [.5,1,2]\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  5. Tuning Regularization Parameters (alpha,lambda).</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  5.1. Tune alpha.</b></div>\n",
    "\n",
    "**Tips:** Based on class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reg_alpha                                       auc\n",
      "0    0.001000  (0.8825902776110934, 0.8384799697154699)\n",
      "1    0.053579  (0.8827765101105249, 0.8379853999988344)\n",
      "2    0.106158  (0.8832475962307849, 0.8375166770723563)\n",
      "3    0.158737  (0.8827204177159876, 0.8389957032452664)\n",
      "4    0.211316   (0.8819419237829805, 0.838982933856221)\n",
      "5    0.263895  (0.8816977163693667, 0.8371289055016309)\n",
      "6    0.316474  (0.8819867012809773, 0.8372788608832871)\n",
      "7    0.369053    (0.88093476339202, 0.8375452674205288)\n",
      "8    0.421632  (0.8815444908724631, 0.8391031538913749)\n",
      "9    0.474211  (0.8814753024250257, 0.8378582929768082)\n",
      "10   0.526789  (0.8811426042809007, 0.8380137049217223)\n",
      "11   0.579368  (0.8809835952946458, 0.8381551718822834)\n",
      "12   0.631947  (0.8803760144680881, 0.8397224351644514)\n",
      "13   0.684526  (0.8803515938517756, 0.8395350359151322)\n",
      "14   0.737105  (0.8793435692692295, 0.8376811108037694)\n",
      "15   0.789684  (0.8795401523438638, 0.8391728378912641)\n",
      "16   0.842263  (0.8789200872826889, 0.8392052025000586)\n",
      "17   0.894842  (0.8797558989251067, 0.8375232981744786)\n",
      "18   0.947421  (0.8786937028760663, 0.8370332844473024)\n",
      "19   1.000000  (0.8794699890480207, 0.8383969561638362)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1,\n",
    "            \"gamma\":1,\"subsample\":0.7,\"colsample_bytree\":.7, \"scale_pos_weight\":1}\n",
    "\n",
    "V_PARAM_NAME = \"reg_alpha\"\n",
    "V_PARAM_VALUES = np.linspace(start=0.001, stop=1, num=20).tolist()\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking .15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  5.2. Tune lambda.</b></div>\n",
    "\n",
    "**Tips:** Based on class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reg_lambda                                       auc\n",
      "0     0.001000  (0.8837629294030112, 0.8377773090342933)\n",
      "1     0.053579  (0.8834974755847746, 0.8384035914936796)\n",
      "2     0.106158  (0.8841805799636513, 0.8375535646739086)\n",
      "3     0.158737  (0.8847404995246787, 0.8366095528781997)\n",
      "4     0.211316  (0.8836092981816227, 0.8386059183125034)\n",
      "5     0.263895  (0.8821506722332023, 0.8379296769712348)\n",
      "6     0.316474  (0.8836651397716764, 0.8370513569434376)\n",
      "7     0.369053   (0.882388942144463, 0.8367691535569829)\n",
      "8     0.421632  (0.8847808117276802, 0.8371616555450355)\n",
      "9     0.474211   (0.883592293060415, 0.8374150080375327)\n",
      "10    0.526789  (0.8836578971826203, 0.8387685700484389)\n",
      "11    0.579368  (0.8832800764386283, 0.8407992397031212)\n",
      "12    0.631947  (0.8823439507992432, 0.8386409162656416)\n",
      "13    0.684526   (0.883278644113819, 0.8407971549564776)\n",
      "14    0.737105   (0.882643406935463, 0.8386381056850116)\n",
      "15    0.789684  (0.8818644813284889, 0.8388627272415266)\n",
      "16    0.842263  (0.8820672408831685, 0.8378400629225178)\n",
      "17    0.894842  (0.8821126934527594, 0.8383878755155532)\n",
      "18    0.947421   (0.8805970918475097, 0.840384160023763)\n",
      "19    1.000000   (0.8814121667075415, 0.840631488780599)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"learning_rate\": 0.5,\"max_depth\":5,\"min_child_weight\":1,\n",
    "            \"gamma\":1,\"subsample\":0.7,\"colsample_bytree\":.8, \"scale_pos_weight\":1,\"reg_alpha\":0.15}\n",
    "\n",
    "V_PARAM_NAME = \"reg_lambda\"\n",
    "V_PARAM_VALUES = np.linspace(start=0.001, stop=1, num=20).tolist()\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  6. Lastly, Reduce Learning Rate and add more trees</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:6px;color:black;padding-left: 60px;font-size:120%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\">\n",
    "<b>  6.1. Reduce Learning rate.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate                                       auc\n",
      "0       0.010000  (0.8524824902942804, 0.8283759555174786)\n",
      "1       0.042222    (0.8559181175211865, 0.82989955699058)\n",
      "2       0.074444  (0.8595689536235975, 0.8315237841377243)\n",
      "3       0.106667  (0.8620043872936567, 0.8324881862476035)\n",
      "4       0.138889  (0.8648855464037115, 0.8342868908063013)\n",
      "5       0.171111  (0.8685044164209602, 0.8361353526161105)\n",
      "6       0.203333  (0.8706196613214325, 0.8390313464586476)\n",
      "7       0.235556  (0.8727173745167882, 0.8397642287729165)\n",
      "8       0.267778  (0.8734459152951304, 0.8397836765291448)\n",
      "9       0.300000   (0.876039324652579, 0.8401660722604714)\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"max_depth\":5,\"min_child_weight\":1,\n",
    "            \"gamma\":1,\"subsample\":0.7,\"colsample_bytree\":.7, \"scale_pos_weight\":1,\"reg_alpha\":0.15,\n",
    "           \"reg_lambda\":1}\n",
    "\n",
    "V_PARAM_NAME = \"learning_rate\"\n",
    "V_PARAM_VALUES = np.linspace(start=0.01, stop=0.3, num=10).tolist()\n",
    "\n",
    "data=xgb_helper(PARAMETERS,V_PARAM_NAME=V_PARAM_NAME,V_PARAM_VALUES=V_PARAM_VALUES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers:\n",
    "- Taking .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;padding-left: 45px;font-size:150%;text-align:left;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>  Full Model</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[23:06:13] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/gbm/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000173b747a8 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000173c14da0 xgboost::gbm::GBTree::ConfigureUpdaters() + 436\n  [bt] (2) 3   libxgboost.dylib                    0x0000000173c1498c xgboost::gbm::GBTree::Configure(std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > > const&) + 964\n  [bt] (3) 4   libxgboost.dylib                    0x0000000173c3087c xgboost::LearnerConfiguration::Configure() + 1016\n  [bt] (4) 5   libxgboost.dylib                    0x0000000173c30b9c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 128\n  [bt] (5) 6   libxgboost.dylib                    0x0000000173b78524 XGBoosterUpdateOneIter + 140\n  [bt] (6) 7   libffi.8.dylib                      0x00000001068e004c ffi_call_SYSV + 76\n  [bt] (7) 8   libffi.8.dylib                      0x00000001068dd790 ffi_call_int + 1256\n  [bt] (8) 9   _ctypes.cpython-39-darwin.so        0x00000001068c016c _ctypes_callproc + 772\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m PARAMETERS\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m5\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_child_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_pos_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m      3\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.3\u001b[39m}\n\u001b[1;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier( tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m,objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,min_child_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m             gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.7\u001b[39m, scale_pos_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,reg_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m      7\u001b[0m            reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m,n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m clf\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical-model.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[0;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[1;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1779\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1780\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/core.py:246\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [23:06:13] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/gbm/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000173b747a8 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000173c14da0 xgboost::gbm::GBTree::ConfigureUpdaters() + 436\n  [bt] (2) 3   libxgboost.dylib                    0x0000000173c1498c xgboost::gbm::GBTree::Configure(std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > > const&) + 964\n  [bt] (3) 4   libxgboost.dylib                    0x0000000173c3087c xgboost::LearnerConfiguration::Configure() + 1016\n  [bt] (4) 5   libxgboost.dylib                    0x0000000173c30b9c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 128\n  [bt] (5) 6   libxgboost.dylib                    0x0000000173b78524 XGBoosterUpdateOneIter + 140\n  [bt] (6) 7   libffi.8.dylib                      0x00000001068e004c ffi_call_SYSV + 76\n  [bt] (7) 8   libffi.8.dylib                      0x00000001068dd790 ffi_call_int + 1256\n  [bt] (8) 9   _ctypes.cpython-39-darwin.so        0x00000001068c016c _ctypes_callproc + 772\n\n"
     ]
    }
   ],
   "source": [
    "PARAMETERS={\"objective\":'binary:logistic',\"eval_metric\":\"auc\",\"max_depth\":5,\"min_child_weight\":1,\n",
    "            \"gamma\":1,\"subsample\":0.7,\"colsample_bytree\":.7, \"scale_pos_weight\":1,\"reg_alpha\":0.15,\n",
    "           \"reg_lambda\":1,\"learning_rate\": 0.3}\n",
    "\n",
    "clf = xgb.XGBClassifier( tree_method=\"gpu_hist\",objective=\"binary:logistic\",eval_metric=\"auc\",max_depth=5,min_child_weight=1,\n",
    "            gamma=1,subsample=0.7,colsample_bytree=.7, scale_pos_weight=1,reg_alpha=0.15,\n",
    "           reg_lambda=1,learning_rate= 0.3,n_estimators=800)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.save_model(\"categorical-model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:1434\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m   1426\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1427\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1433\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 1434\u001b[0m     class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1435\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1436\u001b[0m         output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[1;32m   1437\u001b[0m         ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[1;32m   1438\u001b[0m         validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[1;32m   1439\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1440\u001b[0m         iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[1;32m   1441\u001b[0m     )\n\u001b[1;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m output_margin:\n\u001b[1;32m   1443\u001b[0m         \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m         \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:1044\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m   1002\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1003\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1009\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m   1010\u001b[0m     \u001b[39m\"\"\"Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39m    is used automatically.  For tree models, when data is on GPU, like cupy array or\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39m    cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     iteration_range \u001b[39m=\u001b[39m _convert_ntree_limit(\n\u001b[0;32m-> 1044\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster(), ntree_limit, iteration_range\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1046\u001b[0m     iteration_range \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_iteration_range(iteration_range)\n\u001b[1;32m   1047\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/xgboost/sklearn.py:590\u001b[0m, in \u001b[0;36mXGBModel.get_booster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m    589\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m NotFittedError\n\u001b[0;32m--> 590\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m'\u001b[39m\u001b[39mneed to call fit or load_model beforehand\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster\n",
      "\u001b[0;31mNotFittedError\u001b[0m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred, target_names=[\"0\",\"1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a graph\n",
    "graph = xgb.to_graphviz(clf, num_trees=1)\n",
    "# Or get a matplotlib axis\n",
    "ax = xgb.plot_tree(clf, num_trees=1)\n",
    "# Get feature importances\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:black;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#d9d9d9;overflow:hidden;font-weight:500\">Which parameter does what?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìí 5. Which parameter does what?\n",
    "\n",
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Control Overfitting</b></div>\n",
    "\n",
    "\n",
    "When you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\n",
    "There are in general two ways that you can control overfitting in XGBoost:\n",
    "- **The first way is to directly control model complexity.**\n",
    "  - This includes **max_depth, min_child_weight** and **gamma**.\n",
    "  \n",
    "  \n",
    "- **The second way is to add randomness to make training robust to noise.**\n",
    "  - This includes **subsample** and **colsample_bytree**.\n",
    "  - You can also reduce stepsize **eta**. Remember to increase **num_round** when you do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:120%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Control Overfitting - Code Example : Method 1</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Faster training performance</b></div>\n",
    "\n",
    "There‚Äôs a parameter called **tree_method**, set it to **hist** or **gpu_hist** for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:10px;color:black;margin:0;font-size:150%;text-align:center;display:fill;border-radius:5px;background-color:#f5f2f2;overflow:hidden;font-weight:500\"><b>Handle Imbalanced Dataset</b></div>\n",
    "\n",
    "For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost model, and there are two ways to improve it.\n",
    "\n",
    "- **If you care only about the overall performance metric (AUC) of your prediction**\n",
    "  - Balance the positive and negative weights via **scale_pos_weight**\n",
    "  - Use AUC for evaluation\n",
    "  \n",
    "  \n",
    "- **If you care about predicting the right probability**\n",
    "  - In such a case, you cannot re-balance the dataset\n",
    "  - Set parameter **max_delta_step** to a finite number (say 1) to help convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0aef3ef50817fd087bf5afbc58c91c96bb4be0ad8b75e116879b1dad9351bb93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
